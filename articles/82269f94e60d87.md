---
title: "huggingfaceエコシステムを使ったBERTの事前学習とfine-tuning"
emoji: "✨"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['自然言語処理', 'nlp', 'python', 'pytorch']
published: false
---

近年の自然言語処理を行う上で欠かすことのできない、BERTについて事前学習から下流タスクへのfine-tuningを自身の経験知も交えて解説していきたいと思います。

# 事前学習
BERTの事前学習では穴埋めされたトークン列の一部をマスクしマスクされたトークンが何であったかを予測するタスクを解くことで学習を進めます。

具体的には

1. 入力トークン列のうちランダムに15%のトークンを予測対象として選ぶ。
2. 1で選んだ予測対象トークンのうち、80%を`[MASK]`トークンに置き換え、10%をランダムな単語に変換、10%をオリジナルの単語のままとする。
3. 1で選んだ15%のトークンに対して、オリジナルの単語が何かを予想しクロスエントロピーロスを計算する。

の流れで学習が行われます。

# fine-tuning