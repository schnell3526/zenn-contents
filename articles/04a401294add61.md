---
title: "自然言語処理における知識志向モデルの概観"
emoji: "🎉"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['python', '自然言語処理', 'nlp', '深層学習']
published: false
---
2022年末時点での調査報告になります。

自然言語処理の重要な課題の一つとしてモデルに世界知識をどのように付与するかというものがあります。BERTやGPT等のモデルは事前学習時に文章中からマスクされたトークンを予測するマスク言語モデルや、それまでのトークン列から次のトークン列を予測する自己回帰言語モデル(本来は言語モデルといえばこれの意)によって自然言語をモデル化します。

このようなモデル化手法のメリットはなんと言っても特殊なアノテーションをせずに学習(半教師あり学習)をすることが可能という点にあります。Transformerの並列計算性能の寄与もあり、莫大なデータを用いて言語の特徴を学習できるようになりました。

しかしながら、このような学習手法ではテキストに表れる知識情報のみしか学習することができず、我々が生きている"世界"から得られる情報を効率的に得ることができているとは言いずらいです。また仮にそのような世界知識がテキストから学習できると仮定しても、このような知識は移り変わるものであり、近年大規模化が進むテキスト自然言語処理モデルをその度に学習させないといけないのは非常に効率が悪いといえます。

そこで話は初めの段落に戻りますが、「モデルに世界知識をどのように付与するか」が重要な研究対象の1つとなっております。以下ではモデルに知識を付与する手法として、代表的なもの、個人的に大切だと思ったものを紹介していきます。