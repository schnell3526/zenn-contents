---
title: "Huggingface TransformersのTokenizer API及び、Huggingface Tokenizersの仕様の備忘録"
emoji: "📝"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['自然言語処理']
published: false
---

# TransformersのTokenizer API
Transformersは事前学習モデルを活用することを主用途としていると思いますのでその前提で書きます。

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('schnell/bert-small-spm')

# テキストを単語分割
print(tokenizer.tokenize('日本語のテキストを単語分割する。'))
# ['日本語の', 'テキスト', 'を', '単語', '分割', 'する', '。']

# テキストをBERT用にエンコード
print(tokenizer('日本語のテキストを単語分割する。'))
# {'input_ids': [2, 17413, 4476, 10, 5899, 3990, 29, 6, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}

# 2つのテキストを結合する。(3つのテキスト[SEP]トークンを使ってつなげるようなことはこのAPIからはできない)
print(tokenizer(*['こんにちは','こんばんは']))
# {'input_ids': [2, 4802, 3, 20559, 3], 'token_type_ids': [0, 0, 0, 1, 1], 'attention_mask': [1, 1, 1, 1, 1]}
```

