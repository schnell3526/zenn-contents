---
title: "ã€è‡ªç„¶è¨€èªå‡¦ç†ã€‘BERTã®å˜èªãƒ™ã‚¯ãƒˆãƒ«ã§ã€Œç‹+å¥³-ç”·ã€ã‚’è¨ˆç®—ã—ã¦ã¿ã‚‹"
emoji: "ğŸ‘Œ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["è‡ªç„¶è¨€èªå‡¦ç†", "faiss", "python", "ãƒ™ã‚¯ãƒˆãƒ«", "bert"]
published: true
---

ãƒ™ã‚¯ãƒˆãƒ«ã®è¿‘å‚æ¢ç´¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªfaissã®æ“ä½œå‚™å¿˜éŒ²ã‚’æ›¸ããŸã‹ã£ãŸã®ã§ã™ãŒã€ãã‚Œã ã‘ã ã¨ã¤ã¾ã‚‰ãªã‹ã£ãŸã®ã§ã€Word2Vecç­‰ã§æœ‰åãªå˜èªãƒ™ã‚¯ãƒˆãƒ«ã®æ¼”ç®—ãŒBERTã«ã‚ˆã‚Šç²å¾—ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã§ã‚‚ã§ãã‚‹ã®ã‹èª¿ã¹ã¦ã¿ã¾ã—ãŸã€‚

# äº‹å‰æº–å‚™

::: details ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```shell
python3 -m venv .env
source .env/bin/activate

pip install faiss-cpu transformers numpy
```
:::

::: details BERTã®é™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®å–ã‚Šå‡ºã—
```python
import torch
from transformers import AutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
model = AutoModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')

token_embeds = model.get_input_embeddings().weight.clone()
vocab = tokenizer.get_vocab()
vectors = {}

for idx in vocab.values():
    vectors[idx] = token_embeds[idx].detach().numpy().copy()
```
:::

::: details BERTã®å‹•çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®å–ã‚Šå‡ºã—
ã„ã‚ã‚†ã‚‹contextualized word representaionã¨å‘¼ã°ã‚Œã‚‹ã‚„ã¤ã§ã€å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚ãŸã‚Šã¾ã™ã€‚
æ–‡è„ˆã¾ã§è€ƒãˆã‚‹ã¨å¤§å¤‰ãªã“ã¨ã«ãªã‚‹ã®ã§ã€ä»Šå›ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®32000å€‹ã®èªå½™ã‚’[CLS]ã¨[SEP]ãƒˆãƒ¼ã‚¯ãƒ³ã§æŒŸã‚“ã§å…¥åŠ›ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã‚’ç²å¾—ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚tokenizerã«ãã®ã¾ã¾çªã£è¾¼ã‚ã‚‹ã¨ã‚ˆã‹ã£ãŸã®ã§ã™ãŒã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãŒã‚‰ã¿ã®ä»•æ§˜ãŒé¢å€’ã ã£ãŸã®ã§ãã®ã¾ã¾å˜èªIDã‚’å…¥åŠ›ã—ã¾ã™ã€‚(ã“ã‚Œã®è¨ˆç®—ã«æ‰‹å…ƒã®ç’°å¢ƒã§20åˆ†ãã‚‰ã„ã‹ã‹ã‚Šã¾ã—ãŸã€ã€ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†ã—ãŸã„ã¨ã“ã‚ã§ã™ã­ã€ã€)
```python
vectors_dynamic = {}

from tqdm.auto import tqdm
for idx in tqdm(vocab.values()):
    vectors_dynamic[idx] = model(input_ids=torch.tensor([[2,idx,3]])).last_hidden_state.squeeze(0)[1].detach().numpy().copy()
```
å†è¨ˆç®—ã¯ã‚ã‚“ã©ãã•ã„ã®ã§ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã—ã¾ã—ã‚‡ã†
```python
import pickle

# æ›¸ãè¾¼ã¿
with open('vectors_dynamic.pkl', 'wb') as f:
    pickle.dump(vectors_dynamic, f)

# èª­ã¿å‡ºã—
with open('vectors_dynamic.pkl', 'rb') as f:
    vectors_dynamic = pickle.load(f) 
```

:::

# å˜èªã®ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã—ã¦ã¿ã‚‹

## é™çš„å˜èªåŸ‹ã‚è¾¼ã¿
å…¨ç« ã§æº–å‚™ã‚’ã—ãŸvectorsã¯å˜èªidã¨å˜èªã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹è¾æ›¸ã§ã™ã€‚ä»Šå›ã¯Word2Vecç­‰ã§æœ‰åãªã€Œç‹ - ç”· + å¥³ = å¥³ç‹ã€ã®è¨ˆç®—ã‚’ã‚„ã£ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚
```python
import faiss
index = faiss.IndexFlatL2(len(vectors[0]))
index.add(token_embeds.detach().numpy())


vec = vectors[tokenizer.vocab['ç‹']] - vectors[tokenizer.vocab['ç”·']] + vectors[tokenizer.vocab['å¥³']]

D, I = index.search(vec.reshape((1,768)), 10)
res = [tokenizer.ids_to_tokens[candidate] for candidate in I[0]]
print(res)
# ['ç‹', 'å¥³', 'å¥³ç‹', 'å›½ç‹', 'ç‹å›½', 'ç‹å¦ƒ', 'ç‹å®¶', 'ç¥', '##ç‹', 'å³¶']
```
çµæœã¨ã—ã¦ã¯3ç•ªç›®ã«å¥³ç‹ãŒå‡ºã¦ãã¾ã—ãŸã€‚
ã€Œç‹+å¥³ã€ã§è©¦ã—ã¦ã¿ã¾ã™ã€‚
```python
vec = vectors[tokenizer.vocab['ç‹']] + vectors[tokenizer.vocab['å¥³']]
D, I = index.search(vec.reshape((1,768)), 10)
res = [tokenizer.ids_to_tokens[candidate] for candidate in I[0]]
print(res)
# ['å¥³', 'ç‹', 'ç”·', 'å¥³æ€§', 'å¥³ç‹', 'å¨˜', 'å¥³å­', 'ç¥', 'å§«', 'äºº']
```
ç‹å˜ä½“ã§èª¿ã¹ã¦ã¿ã¾ã™ã€‚
```python
vec = vectors[tokenizer.vocab['ç‹']]
D, I = index.search(vec.reshape((1,768)), 10)
res = [tokenizer.ids_to_tokens[candidate] for candidate in I[0]]
print(res)
# ['ç‹', 'å›½ç‹', 'ç‹å›½', 'ç¥', 'çš‡å¸', 'å¥³ç‹', '##ç‹', 'ç‹å®¶', 'ã‚¤ã‚®ãƒªã‚¹', 'å³¶']
```
ã“ã®æ™‚ç‚¹ã§å¥³ç‹ã¨ã„ã†å˜èªãŒå‰²ã¨ä¸Šä½ã«å‡ºã¦ãã‚‹ã®ã§ã€ã€Œç‹ + å­ã€ã®ãƒ™ã‚¯ãƒˆãƒ«ã®è¿‘å‚ã‚’èª¿ã¹ãŸã„ã¨æ€ã„ã¾ã™ã€‚ç‹å­ãŒä¸Šä½ã«ä¸ŠãŒã£ã¦ã‚‹ã®ãŒç†æƒ³ã§ã™ã‹ã­ã€‚
```python
vec = vectors[tokenizer.vocab['ç‹']] + vectors[tokenizer.vocab['å­']]
D, I = index.search(vec.reshape((1,768)), 10)
res = [tokenizer.ids_to_tokens[candidate] for candidate in I[0]]
print(res)
# ['å­', 'ç‹', 'æ¯å­', '##å­', 'å­ä¾›', 'å¨˜', 'ç‹å­', 'ç‹å›½', 'å¼Ÿ', 'çˆ¶']
```
ç‹å­ãŒä¸Šä½ã«ä¸ŠãŒã£ã¦ãã¾ã—ãŸã€‚ãªã‚“ã¨ãªãã€Œå­ã€ã®å½±éŸ¿ã‚’å¼·ãå—ã‘ã¦ã„ã‚‹ã‚ˆã†ãªæ°—ãŒã—ã¾ã™ã€‚ãƒãƒ«ãƒ ã‚’æ¯”è¼ƒã—ã¦ã¿ã‚‹ã¨
```python
print(f"ç‹ = {np.linalg.norm(vectors[tokenizer.vocab['ç‹']])}")
print(f"å­ = {np.linalg.norm(vectors[tokenizer.vocab['å­']])}")
print(f"å¥³ = {np.linalg.norm(vectors[tokenizer.vocab['å¥³']])}")
print(f"ç”· = {np.linalg.norm(vectors[tokenizer.vocab['ç”·']])}")
# ç‹ = 1.0982342958450317
# å­ = 1.1643147468566895
# å¥³ = 1.134019136428833
# ç”· = 1.125975251197815
```
ã“ã‚Œã¾ã§ã®çµæœã¨ç…§ã‚‰ã—ã‚ã‚ã™ã¨ã€ãƒãƒ«ãƒ ãŒå¤§ãã„æ–¹ã®ãƒ™ã‚¯ãƒˆãƒ«ã®å½±éŸ¿ã‚’å¼·ãå—ã‘ã¦ã„ã‚‹ã‚ˆã†ãªæ°—ãŒã—ã¾ã™ã€‚(ãŸã ã—ãƒãƒ«ãƒ ã‚’ä½¿ã£ã¦æ­£è¦åŒ–ã—ã¦ã‚‚çµæœã¯å¤‰ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚)

## å‹•çš„å˜èªåŸ‹ã‚è¾¼ã¿
å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹å˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ã¦åŒã˜ã‚ˆã†ã«ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã‚’ã—ã¦ã¿ã¾ã™ã€‚
ã¾ãšã¯è¿‘å‚æ¢ç´¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æº–å‚™ã‚’ã—ã¾ã™ã€‚
```python
index = faiss.IndexFlatL2(len(vectors_dynamic[0]))
index.add(np.concatenate([[rep] for rep in vectors_dynamic.values()], axis=0))
```

```python
vec = vectors_dynamic[tokenizer.vocab['ç‹']] + vectors_dynamic[tokenizer.vocab['å¥³']] - vectors_dynamic[tokenizer.vocab['ç”·']]
D, I = index.search(vec.reshape((1,768)), 10)
res = [tokenizer.ids_to_tokens[candidate] for candidate in I[0]]
print(res)
# ['ç‹', 'ç‹ä½', 'ç‹å®®', 'ç‹æœ', 'å¥³', 'å®®', 'å®—', 'ç‹å¦ƒ', 'çš‡æ—', 'å']
```

ã€Œç‹+å­ã€ã§è©¦ã—ã¦ã¿ã‚‹ã¨
```
['ç‹', 'å­', 'ä¼¯', 'ç‹å¦ƒ', 'å®—', 'ä½™', 'ç¥–', 'ä½¿', '##å­', 'ç‹ä½']
```
ã€Œç‹ï¼‹å¥³ã€ã§è©¦ã—ã¦ã¿ã‚‹ã¨
```
['å¥³', 'ç‹', 'ç”·', 'é¡”', 'ç‹ä½', 'ç‹å­', 'å', 'äººæ°‘', 'è³Š', 'æŠ€å¸«']
```
ã“ã‚Œã‚’ã¿ã‚‹ã¨é™çš„ãªå˜èªãƒ™ã‚¯ãƒˆãƒ«ã®æ–¹ãŒãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®è¶³ã—ç®—ã®çµæœãŒç›´è¦³ã«è¿‘ãæ„Ÿã˜ã¾ã™ã­ã€‚

# å‹•çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã‚‹
å…¨ç« ã®çµæœã‹ã‚‰ã€é™çš„å˜èªåŸ‹ã‚è¾¼ã¿ã¨å‹•çš„ã«ç²å¾—ã•ã‚Œã‚‹å˜èªãƒ™ã‚¯ãƒˆãƒ«ã¯å°‘ã—å‹æ‰‹ãŒé•ã†ã‚ˆã†ã«æ„Ÿã˜ã¾ã—ãŸã®ã§å¯è¦–åŒ–ã‚’è©¦ã¿ã¾ã™ã€‚
å¯è¦–åŒ–ã«ã‚ãŸã£ã¦æ‰‹é †ã¯[ã“ã¡ã‚‰](https://zenn.dev/hpp/articles/d347bcb7ed0fc0)ã‚’å‚è€ƒã«ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚é™çš„ãªå˜èªåŸ‹ã‚è¾¼ã¿ã®å ´åˆã®å¯è¦–åŒ–ã‚‚ã“ã¡ã‚‰ã®è¨˜äº‹ã‹ã‚‰å‚ç…§ã§ãã¾ã™ã€‚

```python
import pickle

from transformers import AutoTokenizer
from sklearn.manifold import TSNE
import holoviews as hv
from holoviews import opts


tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
vocab = tokenizer.get_vocab()
with open('vectors_dynamic.pkl', 'rb') as f:
    vectors_dynamic = pickle.load(f) 

tsne = TSNE(n_components=2, random_state=0)
reduced_vectors = tsne.fit_transform(list(vectors_dynamic.values())[:3000])

hv.extension('plotly')

points = hv.Points(reduced_vectors)
labels = hv.Labels({('x', 'y'): reduced_vectors, 'text': [token for token, _ in zip(vocab, reduced_vectors)]}, ['x', 'y'], 'text')

(points * labels).opts(
    opts.Labels(xoffset=0.05, yoffset=0.05, size=14, padding=0.2, width=1500, height=1000),
    opts.Points(color='black', marker='x', size=3),
)
```

çµæœã¯ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã—ãŸã€‚ãƒ‘ãƒƒã¨ã¿ãŸæ„Ÿã˜ä¸Šã®æ–¹ã«ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®ã‚¯ãƒ©ã‚¹ã‚¿ãŒã€å·¦ä¸‹ã®æ–¹ã«æ•°å­—ã®ã‚¯ãƒ©ã‚¹ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ãŒè¦‹ã¦å–ã‚Œã¾ã™ã€‚
![](https://storage.googleapis.com/zenn-user-upload/e422ca65c9b1-20220706.png)

æ•°å­—ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ‹¡å¤§ã—ã¦ã¿ã¾ã—ãŸã€‚æ¡æ•°ãŒè¿‘ã„ã‚‚ã®ãŒçºã¾ã£ã¦ã„ã¾ã™ã­ã€ã¾ãŸéš£ã‚Šåˆã†æ•°å­—ã‚‚è¿‘ãã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ãªæ°—ãŒã—ã¾ã™ã€‚
![](https://storage.googleapis.com/zenn-user-upload/abc130a16022-20220706.png)

å›½åã€åœ°åŸŸåã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚‚å­˜åœ¨ã—ã¾ã—ãŸã€‚ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ã ã‘ã§ã‚‚å›½åã‚„åœ°åŸŸåã¨ã„ã†å¤–éƒ¨çŸ¥è­˜ã¯æš—é»™çš„ã«ç²å¾—ã§ãã¦ã„ãã†ã§ã™ã€‚
![](https://storage.googleapis.com/zenn-user-upload/7bfdabadbc87-20220706.png)

ã“ã®è¾ºã‚Šã¯æ¼¢å­—1å­—ã®ã‚¯ãƒ©ã‚¹ã‚¿ã§ã™ã€‚
![](https://storage.googleapis.com/zenn-user-upload/b27391cd56ce-20220706.png)

ã“ã®è¾ºã‚Šã‚’ã¿ã‚‹ã¨åˆ†ã‹ã‚Šã‚„ã™ã„ã¨æ€ã„ã¾ã™ãŒã€åŒã˜æ–‡è„ˆã§ç™»å ´ã—ãã†ãªå˜èªãŒè¿‘ã„ä½ç½®ã«ã‚ã‚‹ã“ã¨ãŒã¿ã¦å–ã‚Œã¾ã™ã€‚
![](https://storage.googleapis.com/zenn-user-upload/65d59931e074-20220706.png)

ä»Šå›ã®çµæœã‹ã‚‰ã¯ã©ã†ã—ã¦å‹•çš„ã«ç²å¾—ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã ã¨ã€Œç‹+å¥³-ç”·=å¥³ç‹ã€ã¨ã„ã£ãŸæ¼”ç®—ãŒä¸Šæ‰‹ã„ã“ã¨ã„ã‹ãªã‹ã£ãŸã‹ã¯åˆ†ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚
ä½•ã‹æ€ã†ã“ã¨ãŒã‚ã£ãŸæ–¹ã¯ã”é€£çµ¡ã„ãŸã ã‘ã‚‹ã¨å¬‰ã—ã„ã§ã™ã€‚

(è¿½è¨˜)
ã‚ˆãè€ƒãˆãŸã‚‰ã€å‹•çš„ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã‚’å¾—ã‚‹éš›ã«æ–‡è„ˆã¨ã—ã¦ä¸ãˆãŸã®ã¯[CLS]ã€[SEP]ã®ã¿ãªã®ã§Attention Layerã§ã®ãƒ‡ãƒ¼ã‚¿ã®æ··ãœåˆã‚ã›ã‚’è¡Œã†éš›ã®ãƒã‚¤ã‚ºã«ã—ã‹ãªã‚‰ãªã•ãã†ã§ã™ã­ã€‚ã€‚
æœ¬å½“ã¯ã‚‚ã†å°‘ã—ãã¡ã‚“ã¨ã—ãŸæ–‡ã‚’å…¥åŠ›ã—ãŸã„ã§ã™ãŒæ¯”è¼ƒæ¡ä»¶ã‚’æ•´ãˆã‚‹ã®ãŒãªã‹ãªã‹ã«é›£ã—ãã†ã§ã™ã€‚